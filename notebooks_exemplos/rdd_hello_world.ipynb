{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do RDD com Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apache Spark', 'Resilient Distributed Dataset', 'Big Data', 'Hadoop']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\"Apache Spark\", \"Resilient Distributed Dataset\", \"Big Data\", \"Hadoop\"]\n",
    "\n",
    "# parallelize() method is used to create an RDD from a list of elements\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operações básicas com RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de elementos no RDD: 4\n"
     ]
    }
   ],
   "source": [
    "# 1. Contar o número de elementos no RDD\n",
    "print(\"Número de elementos no RDD:\", rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiros elementos: ['Apache Spark', 'Resilient Distributed Dataset', 'Big Data']\n"
     ]
    }
   ],
   "source": [
    "# 2. Exibir os primeiros elementos do RDD\n",
    "print(\"Primeiros elementos:\", rdd.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD transformado para maiúsculas: ['APACHE SPARK', 'RESILIENT DISTRIBUTED DATASET', 'BIG DATA', 'HADOOP']\n"
     ]
    }
   ],
   "source": [
    "# 3. Transformação: Map (convertendo para maiúsculas)\n",
    "rdd_upper = rdd.map(lambda x: x.upper())\n",
    "print(\"RDD transformado para maiúsculas:\", rdd_upper.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD filtrado: ['Resilient Distributed Dataset', 'Big Data']\n"
     ]
    }
   ],
   "source": [
    "# 4. Transformação: Filter (filtrando palavras que contêm 'Data')\n",
    "rdd_filtered = rdd.filter(lambda x: \"Data\" in x)\n",
    "print(\"RDD filtrado:\", rdd_filtered.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras agrupadas do RDD: [['Apache', 'Spark'], ['Resilient', 'Distributed', 'Dataset'], ['Big', 'Data'], ['Hadoop']]\n",
      "Palavras individuais do RDD: ['Apache', 'Spark', 'Resilient', 'Distributed', 'Dataset', 'Big', 'Data', 'Hadoop']\n"
     ]
    }
   ],
   "source": [
    "# 5. Transformação: FlatMap (dividindo strings em palavras)\n",
    "# Somente com map\n",
    "rdd_words = rdd.map(lambda x: x.split(\" \"))\n",
    "print(\"Palavras agrupadas do RDD:\", rdd_words.collect())\n",
    "\n",
    "# Utilizando FlatMap\n",
    "rdd_words = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "print(\"Palavras individuais do RDD:\", rdd_words.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenado: Apache, Spark, Resilient, Distributed, Dataset, Big, Data, Hadoop <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# 6. Redução: Concatenando todas as palavras\n",
    "def concat(a, b):\n",
    "    return a + \", \" + b\n",
    "\n",
    "rdd_reduced = rdd_words.reduce(concat)\n",
    "print(\"Concatenado:\", rdd_reduced, type(rdd_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem de palavras: [('Resilient', 1), ('Dataset', 1), ('Spark', 1), ('Distributed', 1), ('Hadoop', 1), ('Big', 1), ('Apache', 1), ('Data', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 7. Contagem de frequência de palavras\n",
    "rdd_word_pairs = rdd_words.map(lambda word: (word, 1))\n",
    "rdd_word_counts = rdd_word_pairs.reduceByKey(lambda a, b: a + b)\n",
    "print(\"Contagem de palavras:\", rdd_word_counts.collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
